\section{Introduction}
\label{sec:intro}
%Nowadays, advent of consumer friendly and affordable depth-cameras such as Kinect are commercially employed in many applications such as robotics and virtual reality.
%Understanding and interpreting raw data provided by depth-cameras has drawn many attentions of researchers.
%

\begin{comment}
\begin{figure}[t]
	\includegraphics[scale=0.65]{figure/Consist.png}
	\vspace*{-0.6cm} 
	\caption{An illustration of jumping problem in video semantic segmentation. The predicted results of adjacent frames by different methods are shown. Our method generates more temporally consistent results, especially in the highlighted regions.}
	\label{fig:Consist}
	\vspace*{-0.35cm}
\end{figure}
}
\end{comment}

Semantic segmentation as a fundamental part of scene understanding is a task of classifying per pixel of the input with semantic label.
%
For indoor scenes, the cluttered backgrounds, large variety of scenes, object occlusions and various illumination pose a series of challenges for accurate semantic segmentation.
%
In recent years, a great deal of studies have been conducted for indoor scene semantic segmentation, and they can be mainly divided into three groups: semantic segmentation of a single RGB image, semantic segmentation of RGBD images, and multi-task learning methods.
%



\noindent \textbf{Semantic Segmentation of a Single Image.}
%
Fully convolutional network (FCN)~\cite{Long2015} is a pioneering work for pixel-wise segmentation. It first converts existing convolutional neural networks (CNN) constructed from classification to semantic segmentation.
%
To overcome the limitations of FCN that the network limited by a fixed-size receptive field, Noh \emph{et al.} \cite{Noh2015} proposed a novel deconvolution algorithm to segment finer object structures.
%
Bayesian SegNet~\cite{Kendall2015} performs visual scene understanding with a measure of model uncertainty to produce a probabilistic segmentation result.
%
To capture semantic correlations between neighboring patches and exploit patch-patch contextual information, Lin \emph{et al.}~\cite{Lin2016} formulated conditional random fields (CRFs) with CNN-based pairwise potential function. 
%
For generating fine prediction, a multi-path refinement network is proposed to effectively exploit multi-level features and refine the prediction step by step~\cite{Lin2017}.
%

\begin{figure*}[htbp]
%	\vspace{-0.6cm}
	\setlength{\abovecaptionskip}{0pt} 
	\setlength{\belowcaptionskip}{10pt}
	\centering
	\centering
	\includegraphics[scale=0.56]{figure/Pipeline.pdf}
%	\vspace*{-0.8cm} 
	\caption{The diagram of our proposed method. Left: we propagate the ground truth from a labeled frame to its adjacent frames by image warping. Right: the flowchart of our training strategy using temporal consistency. 
	It mainly consists of three steps: 1) The current frame $F_t$ passes through a basic semantic segmentation network to generate the prediction $S_{t}$ 2) Warping the prediction $S_{t-1}$ of previous frame $F_{t-1}$ depends on optical flow generated from PWC-Net \cite{Sun2018} 3) Warped prediction of $F_{t-1}$ and generated PGT joint action on the network by $L_{Consistency}$ and $L_{PGT}$ respectively.
		}
	\label{fig:Pipeline}
	\vspace*{-0.2cm}
\end{figure*}
 

\noindent \textbf{Semantic Segmentation of RGB-Depth Images.}
%
With the popularity of affordable depth-cameras, many techniques have been proposed for semantic segmentation of RGBD images.
%
Gupta \emph{et al.} \cite{Gupta2014} extracted features from RGB and depth data and integrated them for object detection and segmentation.
%
A novel long short-term memorized context fusion (LSTM-CF) model is proposed \cite{Li2016} to fuse contextual information from multiple sources such as RGB images and depth data.
% 
Cheng \emph{et al.} \cite{Cheng2017} rethought the relationship between RGB images and depth, and proposed a locality-sensitive deconvolution network to refine object boundaries of segmentation result as well as a gated fusion layer to combine features of two modes.
%
Park \emph{et al.} \cite{Park2017} proposed a multi-modal feature fusion block for fusing the multi-level RGB-D features and integrate these blocks into RefineNet.


 
\noindent \textbf{Multi-task Learning.} 
%
While semantic segmentation, depth estimation, and other tasks for indoor scene understanding share features, especially at shallow layers, many techniques have been proposed to train networks for multiple tasks in order to complement each other.
%
Eigen and Fergus \cite{Eigen2015} addressed three different tasks including depth prediction, surface normal estimation, and semantic labeling using a single multi-scale convolutional network. 
%
Prediction-and-distillation network (PAD-Net)\cite{Xu2018} predicts a set of intermediate tasks including depth, surface normal, semantic and contour estimations, and then uses the predictions from these tasks as multi-modal distillation modules' inputs for final tasks.  
%
A novel joint Task-Recursive Learning (TRL) \cite{Zhang2018} framework for semantic segmentation and depth estimation is proposed by Zhang \emph{et al}. 
%
In TRL, two tasks are alternately processed in the decoder to improve each other.
% 
Jiao \emph{et al.} \cite{Jiao2018} presented an attention-driven loss for network supervision and a synergy network to learn the information sharing strategies. 
%
%By combining two tasks as well as proposed attention-driven loss, the performance of two tasks are mutually improved.
%
Although the multi-task learning methods improve the segmentation results, they require a lot of supplementary information or even dense-annotated data for other tasks.

Above approaches provide a lot of novel ideas to improve the segmentation results. However, the performance of these methods is greatly suppressed by limited densely annotated training data.
%
And the robustness of obtained model derived from a small amount of labeled data is poor.
%
In addition, obtaining a large amount of annotated semantic labels for images is expensive and time-consuming, such as, annotating a $1024\times2048$ Cityscape frame takes an average of 1.5 hours \cite{cordts2016cityscapes}.
% 
In contrast, it is economical to capture videos, and the temporal correlation between adjacent frames naturally provides more information and constraints for the semantic labels. 
%
Therefore, we propose to improve existing networks for semantic segmentation by exploiting the temporal correlation between video frames in two aspects.

First, with only a few labeled video frames, we propagate these labels to its neighboring frames with geometric transformations and generate a large number of pseudo ground truth (PGT) of unlabeled frames for network training. 
%
Compared to common data augmentation operations, it significantly increases the diversity of training data.
%
There are two main image alignment methods to build correspondence of two images for label propagation: optical flow estimation and parametric model.
%
However, due to optical flow estimation heavily depends on the low-level features of the image to find dense correspondence, it is often inaccurate and computationally expensive.
%
Hence, in our work, the method of parametric model is selected for label propagation.
%
The commonly used parametric model methods are rigid body transformation and homography transformation.
%
Owing to obvious holes in the depth data collected by Kinect, and the corresponding camera external parameters are not provided,
%
we refer to homography transformation for label propagation.
%
Second, instead of computing loss for each annotated image individually, we train a semantic segmentation network with a self-supervised loss function to ensure the temporal consistency between adjacent frames in video sequences.
%
With above two training strategies, our approach improves the results of fully supervised baseline method and achieves state-of-the-art performance on semantic segmentation for indoor scenes. 
%
The rest of this paper is organized as follows.
% 
Our methodology will be introduced firstly.
%
Then, we will report the extensive experimental results as well as analyses. 
%
Finally we will draw a conclusion.
